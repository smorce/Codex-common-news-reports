# ============================================
# Google Colabç”¨ RSSè¨˜äº‹å–å¾—ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰
# result = collect_rss_sources(
#     feed_config=FEED_CONFIG,
#     days=7,           # â† 7æ—¥é–“ã«å¤‰æ›´
#     limit_per_feed=10 # â† å„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰10ä»¶ã«å¤‰æ›´
# )
# ============================================

# 1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
!pip install feedparser requests beautifulsoup4 -q

# 2. ã‚³ãƒ¼ãƒ‰ã®å®Ÿè£…
import re
import json
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional

import feedparser
import requests
from bs4 import BeautifulSoup


def _parse_entry_datetime(entry) -> Optional[datetime]:
    """RSSã‚¨ãƒ³ãƒˆãƒªã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º"""
    try:
        if hasattr(entry, "published_parsed") and entry.published_parsed:
            return datetime(*entry.published_parsed[:6])
        if hasattr(entry, "updated_parsed") and entry.updated_parsed:
            return datetime(*entry.updated_parsed[:6])
    except Exception:
        pass
    return None


def _extract_article_text(entry, url: str, session: requests.Session) -> str:
    """è¨˜äº‹æœ¬æ–‡ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    # 1) ã‚¨ãƒ³ãƒˆãƒªã®summaryã‹ã‚‰å–å¾—
    if hasattr(entry, "summary") and entry.summary:
        return BeautifulSoup(entry.summary, "html.parser").get_text(" ", strip=True)

    # 2) æœ¬æ–‡ãƒšãƒ¼ã‚¸ã‹ã‚‰æŠ½å‡º
    try:
        resp = session.get(url, timeout=15)
        if resp.status_code != 200:
            return ""
        soup = BeautifulSoup(resp.text, "html.parser")

        # meta descriptionã‚’å„ªå…ˆ
        meta_desc = soup.find("meta", attrs={"name": "description"})
        if meta_desc and meta_desc.get("content"):
            return str(meta_desc.get("content")).strip()

        # ãªã‘ã‚Œã°æœ€åˆã®6æ®µè½ã‚’å–å¾—
        paragraphs = soup.find_all("p")
        if paragraphs:
            texts = [p.get_text(" ", strip=True) for p in paragraphs[:6]]
            return "\n".join([t for t in texts if t])
    except Exception as e:
        print(f"âš ï¸ è¨˜äº‹å–å¾—ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
        return ""

    return ""


def collect_rss_sources(
    feed_config: Dict[str, List[str]],
    days: int = 1,
    limit_per_feed: int = 3,
) -> Dict[str, object]:
    """RSSã‚½ãƒ¼ã‚¹ã‚’æ©Ÿæ¢°çš„ã«åé›†ã—ã¦ç”Ÿãƒ‡ãƒ¼ã‚¿JSONã‚’è¿”ã™"""
    session = requests.Session()
    session.headers.update(
        {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36"
        }
    )

    cutoff = datetime.now() - timedelta(days=days)
    sources: List[Dict[str, object]] = []

    print(f"ğŸ“¡ RSSåé›†é–‹å§‹: {len(feed_config)}ã‚«ãƒ†ã‚´ãƒªã€éå»{days}æ—¥é–“ã€å„ãƒ•ã‚£ãƒ¼ãƒ‰æœ€å¤§{limit_per_feed}ä»¶\n")

    for category, feed_urls in feed_config.items():
        print(f"ğŸ“‚ ã‚«ãƒ†ã‚´ãƒª: {category} ({len(feed_urls)}ãƒ•ã‚£ãƒ¼ãƒ‰)")
        
        for feed_url in feed_urls:
            try:
                print(f"  â³ å–å¾—ä¸­: {feed_url}")
                parsed = feedparser.parse(feed_url)
                feed_name = (
                    parsed.feed.title if hasattr(parsed, "feed") and hasattr(parsed.feed, "title") else feed_url
                )
                entries = list(getattr(parsed, "entries", []))

                # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ â†’ æ–°ã—ã„é †ã«ã—ã¦limit
                filtered: List = []
                for e in entries:
                    dt = _parse_entry_datetime(e)
                    if dt is None or dt >= cutoff:
                        filtered.append((dt or datetime.now(), e))
                filtered.sort(key=lambda x: x[0], reverse=True)
                filtered = [e for _dt, e in filtered[:limit_per_feed]]

                for e in filtered:
                    url = getattr(e, "link", None)
                    if not url:
                        continue
                    title = getattr(e, "title", "ç„¡é¡Œ")
                    published_at = _parse_entry_datetime(e)
                    text = _extract_article_text(e, url, session)

                    sources.append(
                        {
                            "feed_name": feed_name,
                            "category": category,
                            "title": title,
                            "url": url,
                            "date": published_at.strftime("%Y-%m-%dT%H:%M:%S") if published_at else None,
                            "text": text[:500] + "..." if len(text) > 500 else text,  # è¡¨ç¤ºç”¨ã«500æ–‡å­—åˆ¶é™
                        }
                    )
                
                print(f"    âœ… {len(filtered)}ä»¶å–å¾—: {feed_name}")
                
            except Exception as e:
                print(f"    âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        print()

    now_utc = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
    return {
        "generated_at": now_utc,
        "site": "tech-blogs/rss-sources",
        "num_articles": len(sources),
        "sources": sources,
    }


# 3. ãƒ•ã‚£ãƒ¼ãƒ‰è¨­å®šï¼ˆfeed.tomlã®å†…å®¹ï¼‰
FEED_CONFIG = {
    "tech_blogs": [
        "https://engineering.atspotify.com/feed/",
        "https://netflixtechblog.com/feed",
        "https://blog.cloudflare.com/rss/",
        "https://blog.google/technology/developers/rss/",
        "https://aws.amazon.com/blogs/aws/feed/",
        "https://devblogs.microsoft.com/python/feed/",
        "https://blog.twitter.com/engineering/en_us/blog.rss",
        "https://medium.com/feed/airbnb-engineering",
    ],
    "ai_ml": [
        "https://ai.googleblog.com/feeds/posts/default",
        "https://openai.com/blog/rss/",
        "https://huggingface.co/blog/feed.xml",
        "https://ai-news.dev/feeds/",
        "https://rss.beehiiv.com/feeds/GcFiF2T4I5.xml",
        "https://thinkingmachines.ai/index.xml",
    ],
    "zenn": [
        "https://zenn.dev/topics/cursor/feed",
        "https://zenn.dev/topics/langchain/feed",
        "https://zenn.dev/topics/mcp/feed",
        "https://zenn.dev/topics/aié§†å‹•é–‹ç™º/feed",
    ],
    "qiita": [
        "http://qiita.com/tags/AI/feed.atom",
        "http://qiita.com/tags/Cursor/feed.atom",
        "http://qiita.com/tags/LangChain/feed.atom",
        "http://qiita.com/tags/LLM/feed.atom",
        "http://qiita.com/tags/mcp/feed.atom",
        "http://qiita.com/tags/AIé§†å‹•é–‹ç™º/feed.atom",
    ],
}

# 4. å®Ÿè¡Œ
print("="*60)
print("ğŸš€ RSSè¨˜äº‹åé›†ãƒ†ã‚¹ãƒˆé–‹å§‹")
print("="*60 + "\n")

# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´å¯èƒ½
result = collect_rss_sources(
    feed_config=FEED_CONFIG,
    days=1,           # éå»1æ—¥é–“ã®è¨˜äº‹
    limit_per_feed=3  # å„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ€å¤§3ä»¶
)

# 5. çµæœè¡¨ç¤º
print("="*60)
print("ğŸ“Š åé›†çµæœ")
print("="*60)
print(f"ç”Ÿæˆæ—¥æ™‚: {result['generated_at']}")
print(f"ç·è¨˜äº‹æ•°: {result['num_articles']}ä»¶\n")

# ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®é›†è¨ˆ
category_counts = {}
for source in result['sources']:
    cat = source['category']
    category_counts[cat] = category_counts.get(cat, 0) + 1

print("ğŸ“ˆ ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ:")
for cat, count in category_counts.items():
    print(f"  â€¢ {cat}: {count}ä»¶")

print("\nğŸ“ å–å¾—è¨˜äº‹ä¸€è¦§ï¼ˆæœ€åˆã®5ä»¶ï¼‰:")
for i, source in enumerate(result['sources'][:5], 1):
    print(f"\n{i}. [{source['category']}] {source['title']}")
    print(f"   ğŸ”— {source['url']}")
    print(f"   ğŸ“… {source['date'] or 'æ—¥ä»˜ä¸æ˜'}")
    print(f"   ğŸ“° {source['feed_name']}")
    if source['text']:
        preview = source['text'][:100].replace('\n', ' ')
        print(f"   ğŸ’¬ {preview}...")

# 6. JSONå‡ºåŠ›ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚‚å¯èƒ½ï¼‰
print("\n" + "="*60)
print("ğŸ’¾ JSONå‡ºåŠ›")
print("="*60)

json_output = json.dumps(result, ensure_ascii=False, indent=2)
print(json_output[:1000] + "\n... (çœç•¥)")

# ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹å ´åˆï¼ˆColabã®å ´åˆã¯å·¦å´ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¤ã‚³ãƒ³ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ï¼‰
with open('rss_sources.json', 'w', encoding='utf-8') as f:
    json.dump(result, f, ensure_ascii=False, indent=2)
print("\nâœ… rss_sources.json ã«ä¿å­˜ã—ã¾ã—ãŸ")